{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee39348",
   "metadata": {},
   "source": [
    "# Data Science Smörgåsbord: Stochatic Gradient Descent Logistic Regression\n",
    "### Kori Thompson\n",
    "Logistic regression is a supervised, classification machine learning algorithm. It is most commonly used for binary classification problems or when the outcome can take one of two values or classes. The model outputs the probability of the instance belonging to each class, the class is then decided using a decision threshold. If the probability of belonging to the positive class, the class of interest, is above the threshold value, then it is predicted to be of the positive class; otherwise, it is predicted to belong to the other class. The most common decision threshold is 0.5 or 50%. To limit the range of values to fall within the valid probability space of 0 to 1, a sigmoid function is used to convert the output of the logistic function to be within the range of 0 to 1. This exercise provides an overview of how the logistic regression algorithm works and how gradient descent is utilized in logistic regression.\n",
    "\n",
    "## Logistic Regression\n",
    "Logistic regression works similarly to linear regression in that it creates a model that computes the weighted sum of input features. However, where linear regression provides the estimate of the output variable, logistic regression provides the estimate of the probability that the instance belongs to a specific class. This is what makes the linear regression a classification algorithm despite having regression in its name. In fact, the formula for a logistic regression looks similar to the formula of a linear regression:\n",
    "$$ y = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2...+\\theta_nx_n)$$\n",
    "We can vectorize this formula as $y = \\sigma(\\theta^T\\mathbf{x})$. \n",
    "Where $\\theta^T$ = the weights or model parameters, $\\mathbf{x}$ = the values of input features, and $\\sigma()$ is the sigmoid function.\n",
    "\n",
    "The sigmoid function is what enables the output of the model to fall within the range of 0 and 1. It is defined as the following:\n",
    "$$\\sigma(z) =  \\frac{1}{1+e^{(-z)}}$$\n",
    "We can define $\\sigma$ as the $(\\theta_0 + \\theta_1 * x_1 + \\theta_2 * x_2...+\\theta_n * x_n)$ or in vectorized form $\\sigma = (\\theta^T\\mathbf{x})$.\n",
    "\n",
    "## Cost Function\n",
    "The cost function of logistic regression can be calculated from the log likelihood. The cost function is as follows:\n",
    "$$\\sum_{i=1}^n\\left[y_i  \\log\\sigma(\\theta^Tx_i) + (1-y_i)\\log\\left(1-\\sigma(\\theta^Tx_i)\\right)\\right]$$\n",
    "Our goal with logistic regression, as with all machine learning algorithms, is to minimize the cost function. To do this, we minimize the loss or the negative log likelihood, which is the same as maximizing the log likelihood.\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "Gradient descent works by simultaneously updating the weights by moving in the opposite direction of the gradient, which will eventually lead to a minimum of the cost function. The model will predict values and calculate the cost function and gradient. It will then update the weights and the model will be retrained until the model converges on a minimum or the gradient becomes very small. Using stochastic gradient descent to minimize the logistic regression cost function, we can use the following expression:\n",
    "$$\\theta_{k+1} = \\theta_k - \\eta_kg_k$$ \n",
    "Here $\\eta_k$ is the step size and $g_k$ is the gradient.\n",
    "To find the gradient, we have to differentiate the cost function. The fully differentiated and regularized gradient becomes:\n",
    "$$g_k = \\mathbf{X}^T(\\mu - y) + 2\\lambda\\theta$$\n",
    "\n",
    "## Implementing Logistic Regression with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0df1859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def genData():\n",
    "    \"\"\"\n",
    "    Generate a 1000 instance dataset for demonstration.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        X: array of input feature values\n",
    "        y: vector of output feature values\n",
    "    \"\"\"\n",
    "    n = 1000\n",
    "    mu1 = np.array([1,1])\n",
    "    mu2 = np.array([-1,-1])\n",
    "    pik = np.array([0.4,0.6])\n",
    "    \n",
    "    X = np.zeros((n,2))\n",
    "    y = np.zeros((n,1))\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        u = np.random.rand()\n",
    "        idx = np.where(u < np.cumsum(pik))[0]\n",
    "    \n",
    "    if (len(idx)==1):\n",
    "        X[i,:] = np.random.randn(1,2) + mu1\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        X[i,:] = np.random.randn(1,2) + mu2\n",
    "        y[i] = 0\n",
    "    return X, y\n",
    "\n",
    "class sdgLogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.num_iters = 100\n",
    "        self.lmbda = 1e-9\n",
    "        \n",
    "        self.eta = 0.001\n",
    "        \n",
    "        self.eps = np.finfo(float).eps\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def logisticRegressionObjective(self, theta, X, y, lmbda):\n",
    "        n = y.shape[0]\n",
    "      \n",
    "        \n",
    "        mu = self.sigmoid(X.dot(theta))\n",
    "        # keep bounds away from 0 and 1\n",
    "        mu = np.maximum(mu, self.eps)\n",
    "        mu = np.minimum(mu, 1-self.eps)\n",
    "        \n",
    "        cost = -(1/n)*np.sum(y*np.log(mu) + (1-y)*np.log(1-mu) + np.sum(lmbda*theta*theta))\n",
    "        \n",
    "        grad = X.T.dot(mu-y) + 2*lmbda*theta\n",
    "        \n",
    "        return cost, grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # initialize theta with random values\n",
    "        theta = np.random.randn(X.shape[1],1)\n",
    "        \n",
    "        for itr in range(self.num_iters):\n",
    "            Jcost, Jgrad = self.logisticRegressionObjective(theta, X, y, self.lmbda)\n",
    "            theta = theta - self.eta * Jgrad\n",
    "            print(f'Iteration {itr}, cost: {Jcost}')\n",
    "        \n",
    "        yPred = 2*(self.sigmoid(X.dot(theta)) >0.5) \n",
    "        yError = np.size(np.where(yPred - y)[0])/float(y.shape[0])\n",
    "        print(f'Classification error: {yError}')\n",
    "        return theta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb9b0c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, cost: 0.6929081183356993\n",
      "Iteration 1, cost: 0.6929077126414211\n",
      "Iteration 2, cost: 0.6929073075197583\n",
      "Iteration 3, cost: 0.6929069029697302\n",
      "Iteration 4, cost: 0.6929064989903586\n",
      "Iteration 5, cost: 0.6929060955806668\n",
      "Iteration 6, cost: 0.6929056927396798\n",
      "Iteration 7, cost: 0.692905290466424\n",
      "Iteration 8, cost: 0.6929048887599281\n",
      "Iteration 9, cost: 0.6929044876192224\n",
      "Iteration 10, cost: 0.6929040870433385\n",
      "Iteration 11, cost: 0.6929036870313097\n",
      "Iteration 12, cost: 0.6929032875821711\n",
      "Iteration 13, cost: 0.6929028886949595\n",
      "Iteration 14, cost: 0.6929024903687148\n",
      "Iteration 15, cost: 0.6929020926024758\n",
      "Iteration 16, cost: 0.6929016953952851\n",
      "Iteration 17, cost: 0.6929012987461861\n",
      "Iteration 18, cost: 0.6929009026542244\n",
      "Iteration 19, cost: 0.6929005071184469\n",
      "Iteration 20, cost: 0.6929001121379019\n",
      "Iteration 21, cost: 0.6928997177116409\n",
      "Iteration 22, cost: 0.6928993238387148\n",
      "Iteration 23, cost: 0.692898930518178\n",
      "Iteration 24, cost: 0.6928985377490852\n",
      "Iteration 25, cost: 0.6928981455304946\n",
      "Iteration 26, cost: 0.6928977538614638\n",
      "Iteration 27, cost: 0.6928973627410535\n",
      "Iteration 28, cost: 0.6928969721683266\n",
      "Iteration 29, cost: 0.6928965821423456\n",
      "Iteration 30, cost: 0.6928961926621768\n",
      "Iteration 31, cost: 0.6928958037268867\n",
      "Iteration 32, cost: 0.6928954153355444\n",
      "Iteration 33, cost: 0.6928950274872198\n",
      "Iteration 34, cost: 0.6928946401809853\n",
      "Iteration 35, cost: 0.6928942534159144\n",
      "Iteration 36, cost: 0.6928938671910821\n",
      "Iteration 37, cost: 0.6928934815055654\n",
      "Iteration 38, cost: 0.6928930963584432\n",
      "Iteration 39, cost: 0.6928927117487954\n",
      "Iteration 40, cost: 0.692892327675704\n",
      "Iteration 41, cost: 0.6928919441382525\n",
      "Iteration 42, cost: 0.692891561135526\n",
      "Iteration 43, cost: 0.6928911786666113\n",
      "Iteration 44, cost: 0.6928907967305958\n",
      "Iteration 45, cost: 0.692890415326571\n",
      "Iteration 46, cost: 0.6928900344536273\n",
      "Iteration 47, cost: 0.6928896541108583\n",
      "Iteration 48, cost: 0.6928892742973595\n",
      "Iteration 49, cost: 0.692888895012226\n",
      "Iteration 50, cost: 0.6928885162545568\n",
      "Iteration 51, cost: 0.6928881380234516\n",
      "Iteration 52, cost: 0.6928877603180115\n",
      "Iteration 53, cost: 0.692887383137339\n",
      "Iteration 54, cost: 0.6928870064805387\n",
      "Iteration 55, cost: 0.692886630346717\n",
      "Iteration 56, cost: 0.6928862547349822\n",
      "Iteration 57, cost: 0.6928858796444419\n",
      "Iteration 58, cost: 0.6928855050742087\n",
      "Iteration 59, cost: 0.6928851310233934\n",
      "Iteration 60, cost: 0.6928847574911119\n",
      "Iteration 61, cost: 0.6928843844764784\n",
      "Iteration 62, cost: 0.6928840119786107\n",
      "Iteration 63, cost: 0.6928836399966274\n",
      "Iteration 64, cost: 0.6928832685296491\n",
      "Iteration 65, cost: 0.6928828975767979\n",
      "Iteration 66, cost: 0.6928825271371964\n",
      "Iteration 67, cost: 0.6928821572099714\n",
      "Iteration 68, cost: 0.6928817877942481\n",
      "Iteration 69, cost: 0.6928814188891552\n",
      "Iteration 70, cost: 0.6928810504938224\n",
      "Iteration 71, cost: 0.6928806826073821\n",
      "Iteration 72, cost: 0.6928803152289656\n",
      "Iteration 73, cost: 0.6928799483577088\n",
      "Iteration 74, cost: 0.6928795819927467\n",
      "Iteration 75, cost: 0.6928792161332175\n",
      "Iteration 76, cost: 0.6928788507782608\n",
      "Iteration 77, cost: 0.6928784859270161\n",
      "Iteration 78, cost: 0.6928781215786268\n",
      "Iteration 79, cost: 0.6928777577322367\n",
      "Iteration 80, cost: 0.6928773943869899\n",
      "Iteration 81, cost: 0.6928770315420347\n",
      "Iteration 82, cost: 0.6928766691965187\n",
      "Iteration 83, cost: 0.6928763073495925\n",
      "Iteration 84, cost: 0.6928759460004067\n",
      "Iteration 85, cost: 0.6928755851481153\n",
      "Iteration 86, cost: 0.6928752247918726\n",
      "Iteration 87, cost: 0.6928748649308338\n",
      "Iteration 88, cost: 0.6928745055641583\n",
      "Iteration 89, cost: 0.6928741466910038\n",
      "Iteration 90, cost: 0.6928737883105309\n",
      "Iteration 91, cost: 0.6928734304219029\n",
      "Iteration 92, cost: 0.6928730730242826\n",
      "Iteration 93, cost: 0.6928727161168353\n",
      "Iteration 94, cost: 0.6928723596987282\n",
      "Iteration 95, cost: 0.6928720037691292\n",
      "Iteration 96, cost: 0.6928716483272083\n",
      "Iteration 97, cost: 0.6928712933721359\n",
      "Iteration 98, cost: 0.6928709389030855\n",
      "Iteration 99, cost: 0.6928705849192314\n",
      "Classification error: 0.001\n"
     ]
    }
   ],
   "source": [
    "X, y = genData()\n",
    "sdg = sdgLogisticRegression()\n",
    "theta = sdg.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
